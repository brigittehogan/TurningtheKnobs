{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Tensorflow Example\n",
    "\n",
    "In this example, we are going to look at a data set with measurements taken on several tumors.  For the given data set, we also have the labels or classifications of each tumor (e.g., benign or malignant).\n",
    "\n",
    "We want to build a Tensorflow model that we can use to predict the label for an unclassified tumor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Keras Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout \n",
    "from keras.utils import to_categorical \n",
    "from keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_file = 'cancer_data.csv'\n",
    "target_file = 'cancer_target.csv'\n",
    "cancer_data=np.loadtxt(data_file,dtype=float,delimiter=',')\n",
    "cancer_target=np.loadtxt(target_file, dtype=float, delimiter=',')\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Data size:  \", cancer_data.shape)\n",
    "print(\"Data Summary:\")\n",
    "print(pd.DataFrame(cancer_data).describe())\n",
    "print(\"Classification Summary: \")\n",
    "print(pd.DataFrame(cancer_target).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************\n",
    "#### Decision Point\n",
    "******************************************\n",
    "This is quite a small data set -- only 569 observations.  Let's split it so that only 15% of the observations are used in the test data set.  That give us 85% for the training data.\n",
    "******************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "test_size = 0.15 \n",
    "seed = 7   #The seed in only for reproducible results\n",
    "data = model_selection.train_test_split(cancer_data, cancer_target, test_size=test_size, random_state=seed)\n",
    "\n",
    "train_data = data[0]\n",
    "test_data = data[1]\n",
    "train_target = data[2]\n",
    "test_target = data[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data \n",
    "scaler.fit(train_data)\n",
    "\n",
    "# Now apply the transformations to the data: \n",
    "x_train = scaler.transform(train_data) \n",
    "x_test = scaler.transform(test_data)\n",
    "\n",
    "# Convert the classes to ‘one-hot’ vector \n",
    "y_train = to_categorical(train_target, num_classes=2) \n",
    "y_test = to_categorical(test_target, num_classes=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************\n",
    "#### Decision Point\n",
    "******************************************\n",
    "Again, because the data set is small, let's use only one hidden layer. \n",
    "We'd like the number of nodes to be between 2 (the number of classifications, i.e., malignant or benign) and 30 (the number of columns in the data).  To keep things simple, I'm just going to take the average of those two numbers:  $\\frac{2+30}{2}=16$\n",
    "\n",
    "******************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "# in the first layer, you must specify the expected \n",
    "#input data shape\n",
    "# here, 30-dimensional vectors. \n",
    "model.add(Dense(30, activation='relu', input_dim=30)) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(16, activation='relu')) \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(2, activation='softmax')) \n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************\n",
    "#### Decision Point\n",
    "******************************************\n",
    "At this point, we need to decide on the optimizer function and loss function that we want to use.\n",
    "\n",
    "Because our model is shallow (only one hidden layer), we should use Stochastic Gradient Descent (SGD).  To keep it simple, I will choose a learning rate of 0.01 (arbitrary), and I'm setting the nesterov parameter.  This parameter will adjust the momentum of descent so that it moves faster on the initial descent and moves slower as it approaches a minimum value.\n",
    "\n",
    "For the loss function, I am choosing _binary_crossentropy_ (binary because we have two classifications and crossentropy but this is a classification model).\n",
    "******************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Configure the Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sgd = SGD(lr=0.01, nesterov=True) \n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************\n",
    "#### Decision Point\n",
    "******************************************\n",
    "Our final decisions deal with how we want the training data fed through the model during the fitting process.\n",
    "\n",
    "\n",
    "Once again, we have a small data set; so, I'm going to allow the batch size be all of the data.   I don't know how many times I should do the forward feed/back propagation.  For now, I'm going to choose 20 just to see how accurate my results can be. \n",
    "******************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fit the Model to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b_size = x_train.shape[0]\n",
    "num_epochs = 20\n",
    "\n",
    "model.fit(x_train, y_train, epochs=num_epochs, batch_size=b_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Apply Model to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(model.predict(x_test), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=b_size) \n",
    "print('\\nAccuracy:  %.3f' % score[1])\n",
    "from sklearn.metrics import confusion_matrix \n",
    "print(confusion_matrix(test_target, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Results\n",
    "\n",
    "These results are okay, but not great.  I am especially concerned that the accuracy at the last epoch is so low.  (Mine was only about 65%, but yours may vary due to randomness and how many time the cells were run). But, we want to know if there is anything we can tweak that will make the results better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-on Activities\n",
    "\n",
    "Try the suggested changes (tweaks), and rerun all of the cells to ensure that all of the variables are cleared between runs.\n",
    "\n",
    "1. Tweak #1:  \n",
    "The first thing I would try is increasing the number of epochs.  SGD is known to be slow to arrive at a good solution.  Let's increase the number of epochs to 100 and rerun the training process.  Was there an improvement?\n",
    "2. Tweak #2:\n",
    "What if I added another hidden layer?  Let's try a second hidden layer (between the first hidden layer and the output layer), and give it 8 nodes.  Did that make the results better or worse?\n",
    "3.  Tweak #3:\n",
    "Let's go back to just one hidden layer.  What happens if we increase the number of nodes from 16 to 28?  (I just chose a number between the number of nodes at the input layer and at the hidden layer.)\n",
    "4.  Tweak #4\n",
    "I liked the results that I saw on the last tweak.  Let's increase the number of nodes on the hidden layer to twice the number in the input layer (i.e., 60).  What does that do?\n",
    "5.  Tweak #5\n",
    "What should we do next?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.1/Keras Py3.7",
   "language": "python",
   "name": "tensorflow210_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
